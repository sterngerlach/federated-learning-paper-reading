
% federated-optimization.tex

\documentclass[dvipdfmx,notheorems,t]{beamer}

\usepackage{docmute}
\input{settings}

\usepackage{ulem}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithmic}

\SetKwInOut{Parameter}{parameters}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\newcommand\AlgoCommentFont[1]{\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{AlgoCommentFont}

\title[論文輪講: Federated Optimization]{論文輪講 \\ Federated Optimization: \\ Distributed Machine Learning for On-Device Intelligence}
\author{杉浦 圭祐}
\institute[松谷研究室]{慶應義塾大学理工学部情報工学科 松谷研究室}
\date{\today}

\begin{document}

\frame{\titlepage}

\section{}

\begin{frame}[t,allowdisplaybreaks,allowframebreaks]{目次}
\tableofcontents
\end{frame}

\section{問題設定}

\begin{frame}{問題の定式化}

\begin{itemize}
	\item 問題の定式化
	\begin{itemize}
		\item 解くべき問題は次のように定式化される
		\begin{equation}
			\min_{\bm{w} \in \mathbb{R}^D} f(\bm{w}), \qquad f(\bm{w}) = \frac{1}{N} \sum_{i = 1}^N f_i(\bm{w})
		\end{equation}
	
		\item 入出力のデータを$\left\{ \bm{x}_i, y_i \right\}_{i = 1}^N$、損失関数を$f_i(\bm{w})$とする
		\item 具体的には、以下のような問題が考えられる
		\begin{eqnarray}
			&& \text{線形回帰}: \nonumber \\
			&& \qquad f_i(\bm{w}) = \frac{1}{2} \left( \bm{x}_i^T \bm{w} - y_i \right)^2, \ y_i \in \mathbb{R} \\
			&& \text{ロジスティック回帰}: \nonumber \\
			&& \qquad f_i(\bm{w}) = - \log \left( 1 + \exp \left( - y_i \bm{x}_i^T \bm{w} \right) \right), \ y_i \in \left\{ -1, 1 \right\} \\
			&& \text{サポートベクタマシン}: \nonumber \\
			&& \qquad f_i(\bm{w}) = \max \left\{ 0, 1 - y_i \bm{x}_i^T \bm{w} \right\}, \ y_i \in \left\{ -1, 1 \right\}
		\end{eqnarray}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{問題の定式化}

\begin{itemize}
	\item 問題の定式化
	\begin{itemize}
		\item 上記は凸最適化問題である
		\newline
		
		\item 線形回帰、ロジスティック回帰、サポートベクタマシンより複雑なモデルにも適用可能 \\
		$\Rightarrow$ 条件付き確率場や、ニューラルネットワーク \\
		$\Rightarrow$ 損失関数$f_i(\bm{w})$が非凸で、複雑な形状をしている場合
		\newline
		
		\item データ数$N$が大き過ぎて、単一のノードで学習を進めるのは不可能 \\
		$\Rightarrow$ 分散処理が必要(データが複数の箇所に分散し、複数の相互接続されたノードで学習を行う) \newline \newline
		$\Rightarrow$ ノード間での通信時間がボトルネックとなる可能性 \\
		$\Rightarrow$ 並列計算の利点を活かすために、モデルの学習を、単一ノードでの計算に適した簡単な問題に分割する必要がある
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{問題設定}

\begin{itemize}
	\item 最新(State-of-the-art)の最適化手法
	\begin{itemize}
		\item シーケンシャルであるため、並列処理には向かない
		\newline
		
		\item 各イテレーションでの処理は非常に高速だが、イテレーションの実行を何度も繰り返す必要がある \\
		$\Rightarrow$ 各イテレーションの実行後に、複数のノード間で通信を行うと、性能が極端に落ちる \\
		$\Rightarrow$ 各イテレーションの実行時間より、ノード間の通信時間の方が遥かに大きい
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{問題設定}

\begin{itemize}
	\item 従来手法における仮定
	\begin{enumerate}
		\item データは各ノードに均等に分散している
		\newline
		
		\item ノード数$K$に比べて、1つのノードが保持しているデータ数の平均$N / K$の方が非常に大きい($K \ll N / K$) \\
		$\Rightarrow$ 大規模なデータセンタに、データが格納されている想定
		\newline
		
		\item 各ノードが、分布をよく表現するデータを保持している \\
		$\Rightarrow$ 各ノードが、\alert{独立同分布}(IID)標本を持っているという前提 \newline \newline
		$\Rightarrow$ 実際には、ノードの地理的な位置によって、各ノードが持つデータが、クラスタに分かれている可能性 \\
		$\Rightarrow$ 各ノードが持つデータが時間的に変動し、ある時点では他のノードと似たようなデータを保持している可能性 \\
		$\Rightarrow$ あるノードに頻繁に現れる特徴が、他のノードでは全く現れない可能性
	\end{enumerate}
\end{itemize}

\end{frame}

\begin{frame}{問題設定}

\begin{itemize}
	\item 従来手法における仮定
	\begin{itemize}
		\item 従来手法では、上記3つの仮定が成立する \\
		$\Rightarrow$ Federated Learningでは、これらの\alert{仮定を全く置かない}
		\newline
		
		\item 従来手法では、最初に、デバイス上のデータを中央のノード(データセンタ等)に集める \\
		$\Rightarrow$ 集めたデータをランダムにシャッフルし、複数の計算ノードに均等な数だけ配分して、学習させる
		\newline
		
		\item Federated Learningでは、デバイス上のデータを中央のノードに送信しない \\
		$\Rightarrow$ 各デバイスと中央のノードとの通信量が大幅に削減 \\
		$\Rightarrow$ ユーザのプライバシーを保護し、セキュリティを向上させる \\
		$\Rightarrow$ データがデバイス上にしか存在しないので、中央のノードが攻撃されても、ユーザのデータが漏洩する危険性がない(攻撃されそうな箇所の候補が減る)
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{問題設定}

\begin{itemize}
	\item Federated Learningでの問題設定
	\begin{itemize}
		\item Federated Learningでは、次のような現実的な仮定を置く
	\end{itemize} \
	
	\begin{enumerate}
		\item \alert{Massively Distributed}: データは、多数のノードに分散 \\
		$\Rightarrow$ ノード数$K$と、1つのノードが保持しているデータ数の平均$N / K$を比較したとき、\color{red}$N / K \ll K$\normalcolor となる可能性
		\newline
		
		\item \alert{Non-IID}: 各ノードが保持するデータは、異なる分布からサンプルされた可能性 \\
		$\Rightarrow$ あるノードが保持しているデータは、データ全体の分布を表現しているとは限らない \\
		$\Rightarrow$ 各ノードが保持するデータは、独立に同一の分布からサンプルされた(IID)とは、仮定し難い
		\newline
		
		\item \alert{Unbalanced}: ノードによって、保持しているデータ数は大きく異なる
	\end{enumerate}
\end{itemize}

\end{frame}

\begin{frame}{問題設定}

\begin{itemize}
	\item Federated Learningでの問題設定
	\begin{itemize}
		\item 上記に加えて、この論文では次のような仮定を置く
	\end{itemize} \
		
	\begin{enumerate}
		\item データは\alert{スパース}である \\
		$\Rightarrow$ ある特徴は、ほんの僅かなノードやデータにしか現れない
		\newline
		
		\item データはモバイル端末上に存在し、プライバシー上の配慮が必要(\alert{Privacy Sensitive}) \\
		$\Rightarrow$ 入出力データ$\left\{ \bm{x}_i, y_i \right\}$はデバイス上で作られる \\
		$\Rightarrow$ 例えば、ユーザが次に入力する単語の予測、ユーザがシェアしそうな写真の予測、ユーザにとってどの通知が重要かの予測
		\newline
		
		\item 多数のデバイスが動作するので、事実上無限の計算能力が得られる \\
		$\Rightarrow$ 但し、各デバイスと中央のノード間の、通信コストによって制限される(バンド幅の制限) \\
		$\Rightarrow$ デバイスと中央のノード間の通信を、いかに減らせるかが、性能向上のための鍵となる
		\newline
		
		\item 差分データ$\bm{\delta} \in \mathbb{R}^D$が、モデルの学習に使用する\alert{唯一の情報}である \\
		$\Rightarrow$ 各デバイスは、1回のRoundにつき、モデルのパラメータの差分$\bm{\delta} \in \mathbb{R}^D$を計算($D$は、モデルのパラメータの次元) \\
		$\Rightarrow$ 差分$\bm{\delta}$が、デバイスから中央のノードにアップロードされる
	\end{enumerate}
\end{itemize}

\end{frame}

\begin{frame}{問題設定}

\begin{itemize}
	\item Federated Learningで送信される差分データ$\bm{\delta}$について
	\begin{itemize}
		\item $\bm{\delta}$は、ユーザのプライベートな情報を依然として含むかもしれないが、訓練データ$\left\{ \bm{x}_i, y_i \right\}$に比べれば無視できるほど小さい
		\newline
		
		\item モデルのパラメータの差分ベクトル$\bm{\delta}$のサイズは、訓練データのサイズには関係ない \\
		$\Rightarrow$ 訓練データが巨大(例えばユーザが撮影した動画)であっても、データそのものではなく、差分データのみを中央のノードに送信すればよいため、\alert{通信量が大幅に削減}できる
		
		\framebreak
		
		\item 差分ベクトル$\bm{\delta}$の使途は、元の訓練データ$\left\{ \bm{x}_i, y_i \right\}$に比べて限られる \\
		$\Rightarrow$ モデルの訓練以外に、殆ど使い道がない \\
		$\Rightarrow$ 各デバイスから送信された差分データは、モデルの訓練に使用した後は、破棄してよい \newline \newline
		$\Rightarrow$ ユーザにとっては、アップロードしたデータが、モデルの訓練にしか使われない(想定外の方法で使われない)ことが分かっているので、安心できる(\alert{プライバシーの保護}につながる) \newline \newline
		$\Rightarrow$ モデルを訓練する側にとっては、ユーザのデータを保存する際の負担が軽減される \\
		$\Rightarrow$ 訓練データであれば、漏洩しないように厳重に管理する必要がある \\
		$\Leftrightarrow$ 差分データであれば、万が一不正にアクセスされても、ユーザの個人情報が漏洩することはない
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{問題設定}

\begin{itemize}
	\item 論文で提案された訓練アルゴリズムについて
	\begin{itemize}
		\item Federated Learningのための訓練アルゴリズムを新たに設計した \\
		$\Leftarrow$ \alert{Massively Distributed}、\alert{Non-IID}、\alert{Unbalanced}
		\newline
		
		\item 新たな訓練アルゴリズムによって、比較的少ないRound数(通信量)で、モデルのパラメータを収束させることができた
	\end{itemize}
\end{itemize}

\end{frame}

\section{基本的な最適化アルゴリズム}

\begin{frame}{基本的な最適化アルゴリズム}

\begin{itemize}
	\item 解くべき問題は、次のように定式化された
	\begin{itemize}
		\item $D$はモデルのパラメータの次元数、$N$は訓練データ数
		\item $\bm{w} \in \mathbb{R}^D$はパラメータベクトル、$f_i(\bm{w})$は損失関数
		\begin{equation}
			\min_{\bm{w} \in \mathbb{R}^D} f(\bm{w}), \qquad f(\bm{w}) = \frac{1}{N} \sum_{i = 1}^N f_i(\bm{w})
		\end{equation}
	\end{itemize} \
	
	\item ベースラインアルゴリズム
	\begin{itemize}
		\item 基本的なアルゴリズムとして、以下を紹介する
		\newline
		\item \alert{勾配降下法}(GD; Gradient Descent)
		\item \alert{確率的勾配降下法}(SGD; Stochastic Gradient Descent)
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{基本的な最適化アルゴリズム}

\begin{itemize}
	\item 勾配降下法(GD; Gradient Descent)
	\begin{itemize}
		\item パラメータの更新式は次のようになる
		\begin{equation}
			\bm{w}^{t + 1} = \bm{w}^t - h_t \nabla f(\bm{w}^t)
		\end{equation}
		
		\item $\nabla f(\bm{w}^t)$は次のように定義される
		\begin{eqnarray}
			\nabla f(\bm{w}^t) &\equiv& \left. \frac{\partial}{\partial \bm{w}} f(\bm{w}) \right|_{\bm{w} = \bm{w}^t} \\
			&=& \frac{1}{N} \sum_{i = 1}^N \left. \frac{\partial}{\partial \bm{w}} f_i(\bm{w}) \right|_{\bm{w} = \bm{w}^t} \\
			&=& \frac{1}{N} \sum_{i = 1}^N \nabla f_i(\bm{w}^t)
		\end{eqnarray}
		損失関数$f(\bm{w})$のパラメータ$\bm{w}$による勾配の、$\bm{w} = \bm{w}^t$における値
		
		\item $h_t > 0$は学習率(ステップサイズ)
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{基本的な最適化アルゴリズム}

\begin{itemize}
	\item 勾配降下法(GD; Gradient Descent)の問題点
	\begin{itemize}		
		\item 勾配$\nabla f(\bm{w}^t)$を求めるためには、$N$個の各データについての勾配$\nabla f_i(\bm{w}^t)$を計算する必要がある \\
		$\Rightarrow$ 1回のパラメータ更新のために、全データを処理する必要がある \\
		$\Rightarrow$ データ数$N$は非常に大きいため、勾配の計算に時間が掛かり過ぎる \newline \newline
		$\Rightarrow$ 勾配降下法は、遅すぎて使い物にならない
		\newline
		
		\item モメンタム項を加えることで、アルゴリズムを高速化できる \\
		$\Rightarrow$ 但し、1回のパラメータ更新のために、全データを処理する必要はある \\
		$\Rightarrow$ モメンタム項を加えても、やはり使い物にならない
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{基本的な最適化アルゴリズム}

\begin{itemize}
	\item 確率的勾配降下法(SGD; Stochastic Gradient Descent)
	\begin{itemize}
		\item パラメータの更新式は次のようになる
		\begin{equation}
			\bm{w}^{t + 1} = \bm{w}^t - h_t \nabla f_{i_t}(\bm{w}^t)
		\end{equation}
		
		\item $\nabla f_{i_t}(\bm{w}^t)$は次のように定義される
		\begin{eqnarray}
			\nabla f_{i_t}(\bm{w}^t) = \left. \frac{\partial}{\partial \bm{w}} f_{i_t}(\bm{w}) \right|_{\bm{w} = \bm{w}^t}
		\end{eqnarray}
		
		\item $i_t$は、$1$から$N$の中から適当に選択されたインデックス \\
		$\Rightarrow$ 時刻$t$では、$i_t$番目のデータ$\left\{ \bm{x}_{i_t}, y_{i_t} \right\}$とパラメータ$\bm{w}^t$を用いて、損失関数$f_{i_t}(\bm{w}^t)$を計算 \\
		$\Rightarrow$ 損失関数の勾配$\nabla f_{i_t}(\bm{w}^t)$を求めて、パラメータ$\bm{w}$を勾配の方向に更新
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{基本的な最適化アルゴリズム}

\begin{itemize}
	\item 確率的勾配降下法(SGD; Stochastic Gradient Descent)
	\begin{itemize}
		\item 1回のパラメータ更新のためには、1つのデータ点に対する勾配$\nabla f_{i_t}(\bm{w}^t)$だけを求めればよい \\
		$\Rightarrow$ 勾配降下法とは異なり、全データを処理する必要がない
		\newline
		
		\item 1つのデータ点に対する勾配の期待値は、損失関数$f(\bm{w})$の勾配の不偏推定量となっている \\
		$\Rightarrow$ $\mathbb{E} \left[ \nabla f_{i_t}(\bm{w}) \right] = \nabla f(\bm{w}^t)$であり、この手法が正当化される \newline \newline
		$\Rightarrow$ 実際には、データ点のサンプリングによって、(真の勾配に対して)ノイズが加わった勾配が得られるため、パラメータの収束が遅くなる \\
		$\Rightarrow$ 学習率(ステップサイズ)$h_t$の設定が重要になる
		\newline
		
		\item 勾配の計算に用いるデータ点$i_t$を、毎回ランダムに選ぶのではなく、全データをランダムな順で取り出し、その勾配を求めてパラメータを更新していく手法がある
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{基本的な最適化アルゴリズム}

\begin{itemize}
	\item GDとSGDとの比較
	\begin{itemize}
		\item GDではパラメータの収束が速い $\Leftrightarrow$ SGDは収束が遅い
		\newline
		
		\item GDでは各イテレーションの計算に非常に時間が掛かる \\
		$\Leftarrow$ 各イテレーションにおいて、全データを処理する必要がある \\
		$\Leftrightarrow$ SGDでは各イテレーションの計算は高速であり、計算時間はデータ数$N$に依存しない
		\newline
		
		\item 今回解こうとしている問題では、パラメータの精度はそこまで高くなくてもよい \\
		$\Rightarrow$ SGDで十分である(極端な場合には、全データを1回ずつ処理するだけで、パラメータが収束) \\
		$\Leftarrow$ GDの場合は、全データを1回ずつ処理して、ようやくパラメータを1回更新できる
	\end{itemize}
\end{itemize}

\end{frame}

\section{ランダム化された最適化アルゴリズム}

\begin{frame}{ランダム化された最適化アルゴリズム}

\begin{itemize}
	\item ランダム化された座標降下法(RCD; Randomized Coordinate Descent)
	\begin{itemize}
		\item パラメータの更新式は次のようになる
		\begin{equation}
			\bm{w}^{t + 1} = \bm{w}^t - h_{j_t} \nabla_{j_t} f(\bm{w}^t) \bm{e}_{j_t}
		\end{equation}
		
		\item $j_t$は、$1$から$D$(パラメータの次元数)の中から適当に選択された次元
		\item $h_{j_t}$は学習率(ステップサイズ)、$\bm{e}_{j_t} \in \mathbb{R}^D$は次元$j_t$方向の標準基底ベクトル
		\item 勾配$\nabla_j f(\bm{w}^t)$は次のように定義される
		\begin{eqnarray}
			\nabla_j f(\bm{w}^t) &=& \frac{\partial}{\partial w_j} f(\bm{w}^t) \\
			&=& \frac{1}{N} \sum_{i = 1}^N \left. \frac{\partial}{\partial w_j} f_i(\bm{w}) \right|_{\bm{w} = \bm{w}^t} \\
			&=& \frac{1}{N} \sum_{i = 1}^N \nabla_{w_j} f_i(\bm{w}^t)
		\end{eqnarray}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{ランダム化された最適化アルゴリズム}

\begin{itemize}
	\item ランダム化された座標降下法(RCD; Randomized Coordinate Descent)
	\begin{itemize}
		\item 最適化問題$\displaystyle \min_{\bm{w} \in \mathbb{R}^D} f(\bm{w})$を、幾つかの部分問題に分割
		\newline
		
		\item $f(\bm{w})$を、$j \neq j_t$であるような変数については固定したまま、ある1つの変数$j_t \in \left\{ 1, \ldots, D \right\}$について最小化する \\
		$\Rightarrow$ 一度に1つの座標を最適化するため、\alert{座標降下法}とよばれる \newline \newline
		$\Rightarrow$ 一般に、変数の部分集合について同時に最小化を行うアルゴリズムを、\alert{ブロック座標降下法}という
		\newline
		
		\item 座標降下法は、ある1つの変数が、他の変数の最適値に影響を与えない場合に効果を発揮 \\
		$\Rightarrow$ 「深層学習」の8.7.2節を参照
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{ランダム化された最適化アルゴリズム}

\begin{itemize}
	\item SDCA; Stochastic Dual Coordinate Ascent
	\begin{itemize}
		\item 正則化項$\psi(\bm{w})$を付加した、以下の最適化問題を考える
		\begin{equation}
			\min_{\bm{w} \in \mathbb{R}^D} f(\bm{X} \bm{w}) + \psi(\bm{w}), \qquad f(\bm{w}) = \frac{1}{N} \sum_{i = 1}^N f_i(y_i, \bm{x}_i^T \bm{w})
		\end{equation}
		
		\item 上記の\alert{双対問題}(Dual problem)は、\alert{Fenchel双対定理}から、次のようになる($K$はデータ$\bm{x}$の次元数)~\cite{suzuki_2013}
		\begin{equation}
			- \min_{\bm{u} \in \mathbb{R}^N} f^*(\bm{u}) + \psi^* \left( -\frac{\bm{X}^T \bm{u}}{N} \right)
		\end{equation}
		
		\item 但し、$f^*$と$\psi^*$は、それぞれ$f$と$\psi$の\alert{ルジャンドル変換}である
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{ランダム化された最適化アルゴリズム}

\begin{itemize}
	\item SDCA; Stochastic Dual Coordinate Ascent
	\begin{itemize}
		\item $f(\bm{x})$の\alert{ルジャンドル変換}$f^*(\bm{y})$とは、関数$f(\bm{x})$の変数を、その微分$\bm{y} = \bm{x}'$に置き換えた関数であり、次のように定義される(関数$f(\bm{x})$を、その傾きの情報から捉えた関数)
		\begin{equation}
			f^*(\bm{y}) = \sup_{\bm{x}} \left\{ \bm{x}^T \bm{y} - f(\bm{x}) \right\}
		\end{equation}
		
		\item 双対問題(Dual problem)とは、最適化問題における\alert{主問題}(Primary problem)の\alert{補問題}であり、主問題と双対問題は表裏一体の関係にある
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{ランダム化された最適化アルゴリズム}

\begin{itemize}
	\item SDCA; Stochastic Dual Coordinate Ascent
	\begin{itemize}		
		\item 例えば、以下の問題$P$(主問題)を、次のように定める
		\begin{equation}
			\min_{\bm{x}} \bm{c}^T \bm{x} \quad \mathrm{s.t.} \quad \bm{A} \bm{x} = \bm{b}, \bm{x} \ge 0
		\end{equation}
		
		\item 一方の問題$D$(双対問題)は、次のようになる
		\begin{equation}
			\max_{\bm{y}} \bm{b}^T \bm{y} \quad \mathrm{s.t.} \quad \bm{A}^T \bm{y} \le \bm{c}
		\end{equation}
		
		\item 双対定理より、問題$P$に最適解$\bm{x}^*$が存在すれば、問題$D$にも最適解$\bm{y}^*$が存在して、$\bm{c}^T \bm{x}^* = \bm{b}^T \bm{y}^*$が成立
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{ランダム化された最適化アルゴリズム}

\begin{itemize}
	\item SDCA; Stochastic Dual Coordinate Ascent
	\begin{itemize}		
		\item 今回の場合、問題$P$(主問題)は、(正則化項付きの)\alert{損失関数の最小化}である
		\item 問題$P$の代わりに双対問題$D$を解くことができる
		\item このとき、問題$P$の解が得られるので、最適なパラメータが求まる
		\newline
		
		\item SDCAは、このような考え方に基づくアルゴリズムである(詳細は省略)
		\newline
		
		\item 主問題と双対問題を、交互に解くアルゴリズムも考えられる
		\newline
		
		\item 以降のスライドでは、勾配$\nabla f(\bm{w})$の推定値に含まれる\alert{ノイズ}を軽減するアルゴリズムをみていく
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{分散を抑えた確率的勾配降下法}

\begin{itemize}
	\item SAG; Stochastic Average Gradient
	\begin{itemize}
		\item 確率的勾配の平均を取るアルゴリズム
		\item SAGは、GD(勾配降下法)とSGD(確率的勾配降下法)の中間に位置
		\newline
		
		\item SAGの各イテレーションでは、$i_t \in \left\{ 1, \ldots, N \right\}$を選択したうえで、次の処理が実行される
		\begin{eqnarray}
			\bm{w}^{t + 1} &=& \bm{w}^t - \frac{\alpha_t}{N} \sum_{i = 1}^N \bm{y}_i^t \\
			&=& \bm{w}^t - \frac{\alpha_t}{N} \left[ \sum_{i = 1}^N \bm{y}_i^{t - 1} + \left( \bm{y}_{i_t}^t - \bm{y}_{i_t}^{t - 1} \right) \right] \\
			&=& \bm{w}^t - \frac{\alpha_t}{N} \left[ \sum_{i = 1}^N \bm{y}_i^{t - 1} + \left( \nabla f_{i_t}(\bm{w}^t) - \bm{y}_{i_t}^{t - 1} \right) \right]
		\end{eqnarray}
		
		\item 但し、$\bm{y}_i^t$は次のように定義される
		\begin{equation}
			\bm{y}_i^t = \left\{ \begin{array}{ll} \nabla f_i(\bm{w}^t) & (i = i_t) \\ \bm{y}_i^{t - 1} & (\text{それ以外のとき}) \end{array} \right.
		\end{equation}
		
		\item 勾配$\nabla f_i(\bm{w}^t)$は次のように定義される
		\begin{equation}
			\nabla f_i(\bm{w}^t) \equiv \left. \frac{\partial}{\partial \bm{w}} f_i(\bm{w}) \right|_{\bm{w} = \bm{w}^t}
		\end{equation}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{分散を抑えた確率的勾配降下法}

\begin{itemize}
	\item SAG; Stochastic Average Gradient
	\begin{itemize}
		\item GDでは、現在のパラメータ$\bm{w}^t$を基に、\alert{全てのデータについて}勾配$\nabla f_i(\bm{w}^t)$を計算 \\
		$\Rightarrow$ その平均$\sum_{i = 1}^N \nabla f_i(\bm{w}^t)$(\alert{Full gradient})を使って、パラメータ$\bm{w}$を更新 \\
		$\Rightarrow$ 1回のパラメータの更新にはFull gradientが必要であり、全てのデータを使って計算するため、非常に処理が重い
		\newline
		
		\item SGDでは、データ点$i \in \left\{ 1, \ldots, N \right\}$についてのみ(\alert{1つのデータについてのみ})、勾配$\nabla f_i(\bm{w}^t)$を計算 \\
		$\Rightarrow$ その勾配$\nabla f_i(\bm{w}^t)$を使って、パラメータ$\bm{w}$を更新 \\
		$\Rightarrow$ 1点における勾配$\nabla f_i(\bm{w}^t)$を用いて、$\sum_{i = 1}^N \nabla f_i(\bm{w}^t)$を近似 \\
		$\Rightarrow$ 1回のパラメータの更新に必要な、計算量を少なく抑えられる
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{分散を抑えた確率的勾配降下法}

\begin{itemize}
	\item SAG; Stochastic Average Gradient
	\begin{itemize}		
		\item SAGでは、全データに対する勾配(Full gradient)を、\alert{少しずつ更新していく} \\
		$\Rightarrow$ ある1つのデータ点$i_t \in \left\{ 1, \ldots, N \right\}$について、現在のパラメータ$\bm{w}^t$の下で勾配$\nabla f_{i_t}(\bm{w}^t)$を計算 \\
		$\Rightarrow$ 新しく求めた勾配$\nabla f_{i_t}(\bm{w}^t)$を使って、以下の式でFull gradientを更新 \\
		\begin{equation}
			\sum_{i = 1}^N \bm{y}_i^t = \sum_{j \neq i_t} \bm{y}_j^{t - 1} + \nabla f_{i_t}(\bm{w}^t)
		\end{equation}
		$\Rightarrow$ 更新されたFull gradientを使って、パラメータ$\bm{w}$を更新
		\newline
		
		\item SAGでは、計算される勾配にはバイアスが含まれる \\
		$\Rightarrow$ 後述の\alert{SAGA}で計算される勾配の期待値は、勾配$f(\bm{w})$の不偏推定量に一致
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{分散を抑えた確率的勾配降下法}

\begin{itemize}
	\item SAG; Stochastic Average Gradient
	\begin{itemize}		
		\item SAGでは、各データにおける勾配の履歴(現在のFull gradient)、従って$\left\{ \bm{y}_i^t \right\}_{i = 1}^N$を記憶しなければならないことが分かる
		\newline
		
		\item 比較的小規模のニューラルネットの学習であっても、勾配を記憶するためのメモリ使用量が大きいため、アルゴリズムは使い物にならなくなる
		\newline
		
		\item GDの速い収束と、SGDの速い計算時間という、双方の利点を受け継いだアルゴリズム
		\newline
		
		\item SAGの改良版として、SAGAアルゴリズムが存在(詳細は省略)
		\item 因みに、SAGAが何の略称なのかは不明
		\newline
		
		\item SAGAにおけるパラメータの更新式は次のようになる~\cite{stochastic_2017}
		\begin{eqnarray}
			\bm{w}^{t + 1} &=& \bm{w}^t - \alpha_t \left[ \frac{1}{N} \sum_{i = 1}^N \bm{y}_i^{t - 1} + \left( \bm{y}_{i_t}^t - \bm{y}_{i_t}^{t - 1} \right) \right] \\
			&=& \bm{w}^t - \alpha_t \left[ \frac{1}{N} \sum_{i = 1}^N \bm{y}_i^{t - 1} + \left( \nabla f_{i_t}(\bm{w}^t) - \bm{y}_{i_t}^{t - 1} \right) \right]
		\end{eqnarray}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{分散を抑えた確率的勾配降下法}

\begin{itemize}
	\item SVRG; Stochastic Variance Reduced Gradient
	\begin{itemize}
		\item 二重ループの最適化アルゴリズムである
		\item 外側のループでは、Full gradient(全データについての勾配の平均)$\nabla f(\bm{w}^t)$を計算
		\begin{equation}
			\nabla f(\bm{w}^t) = \frac{1}{N} \sum_{i = 1}^N \nabla f_i(\bm{w}^t) = \frac{1}{N} \sum_{i = 1}^N \left. \frac{\partial}{\partial \bm{w}} f_i(\bm{w}) \right|_{\bm{w} = \bm{w}^t}
		\end{equation}
		
		\item 内側のループでは、インデックス$i \in \left\{ 1, \ldots, N \right\}$を選択し、次の式を用いてパラメータを更新していく
		\begin{equation}
			\bm{w} = \bm{w} - h \left[ \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right]
		\end{equation}
		
		\item 確率的勾配$\nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t)$は、$\bm{w}$と$\bm{w}^t$における勾配の変化$\nabla f(\bm{w}) - \nabla f(\bm{w}^t)$を\alert{推定するための項}
		\newline
		
		\item SVRGが完全な勾配$\nabla f(\bm{w}^t)$を計算している間、パラメータは一度も更新されないが、同じ時間で、SGDではパラメータが$N$回更新される \\
		$\Rightarrow$ 最初は、SGDの方が学習が進むことが予測される
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{分散を抑えた確率的勾配降下法}

\begin{algorithm}[H]
	\DontPrintSemicolon
	\caption{SVRG; Stochastic Variance Reduced Gradient~\cite{stochastic_2017}}
	\label{alg:svrg}
	\Parameter{$m = $ number of stochastic steps per epoch, $h = $ stepsize (learning rate)}
	\begin{algorithmic}[1]
		\FOR{$s = 0, 1, \ldots$}
			\STATE{Compute and store full gradient $\nabla f(\bm{w}^t) = \frac{1}{N} \sum_{i = 1}^N \nabla f_i(\bm{w}^t)$} \tcp*{Full pass through data}
			\STATE{Set $\bm{w} = \bm{w}^t$}
			\FOR{$t = 1$ \TO $m$}
				\STATE{Pick $i \in \left\{ 1, \ldots, N \right\}$, uniformly at random}
				\STATE{Update using $\bm{w} = \bm{w} - h \left[ \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right]$} \tcp*{Stochastic update}
			\ENDFOR
			\STATE{$\bm{w}^{t + 1} = \bm{w}$}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\end{frame}

\begin{frame}{分散を抑えた確率的勾配降下法}

\begin{itemize}
	\item アルゴリズムについての補足
	\begin{itemize}
		\item SGDと比較するとSVRGの性能は良く、各イテレーションでの分散が小さい
		\newline
		
		\item SVRGと、ランダム化された座標降下法(RCD; Randomized Coordinate Descent)とを結びつけるアルゴリズムが存在
		\item SAGAが提唱された論文では、SAGAはSAGとSVRGの中間に位置付けられている
		\item SVRG、SAGA、SAG、GDを一般化したアルゴリズムが登場している
	\end{itemize}
\end{itemize}

\end{frame}

\section{Federated Learningのための最適化アルゴリズム}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item Federated Learningのための最適化アルゴリズム
	\begin{itemize}
		\item \alert{SVRG}(Stochastic Variance Reduced Gradient)アルゴリズムと、\alert{DANE}(Distributed Approximate Newton)アルゴリズムを調べる
		\item SVRGとDANEは一見無関係にみえるが、実は深く関連し合う
		\newline
		
		\item SVRGを改良した(Naiveな)\alert{Federated SVRG}について説明する
		\item (Naiveな)Federated SVRGを更に改良したアルゴリズムを、新たに提案する \\
		$\Leftarrow$ 各デバイスに保存された訓練データの個数、訓練データのスパース性、各デバイス上の訓練データのパターンの相違について考慮
		\newline
		
		\item Federated Learningでは、訓練データが\alert{Massively Distributed}、\alert{Non-IID}、\alert{Unbalanced}であるという仮定を置く
		\item これに加え、\alert{スパース性}、\alert{Privacy Sensitive}などの仮定を設けた
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item Federated Learningのためのアルゴリズムに必要な特徴
	\begin{enumerate}
		\item アルゴリズムの開始時に、パラメータが既に最適値であったなら、そのアルゴリズムを何度実行しても、パラメータの値が変化しない \label{enum:fl-algorithm-feature-a}
		\newline
		\item 訓練データが単一のノードにしかないとき、パラメータが収束するまでに必要な、中央のノードとの通信回数は$\mathcal{O}(1)$に抑えられること \label{enum:fl-algorithm-feature-b}
		\newline
		\item データの各特徴が、単一のノードにしか現れないとき(解こうとしている問題が完全に分離され、各デバイスがパラメータの一部を学習しているとき)、$\mathcal{O}(1)$回の通信回数の後に、パラメータが収束すること \label{enum:fl-algorithm-feature-c} \newline \newline
		$\Leftarrow$ データの各次元が、ある1つのノードでは$1$になるが、他の全てのノードでは$0$になるような場合
		\newline
		\item 各ノードが完全に同一なデータセットを有するとき、$\mathcal{O}(1)$回の通信回数の後に、パラメータが収束すること \label{enum:fl-algorithm-feature-d}
	\end{enumerate}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item Federated Learningのためのアルゴリズムに必要な特徴
	\begin{itemize}
		\item 「収束する」とは、「十分に精度のある適当な解が得られる」ことを意味している \\
		$\Rightarrow$ $\mathcal{O}(1)$回とは、各デバイスと中央のノード間で、\alert{たった1度だけ}やり取りすることに相当
		\newline
		
		\item (\ref{enum:fl-algorithm-feature-a})は、全ての最適化問題において、考慮する価値がある設定
		\item (\ref{enum:fl-algorithm-feature-b})と(\ref{enum:fl-algorithm-feature-c})は、Federated Learningにおける極端なケース
		\item (\ref{enum:fl-algorithm-feature-d})は、従来の最適化問題における設定(中央の少数のノードが多量のデータを保持している状況) \\
		$\Rightarrow$ (\ref{enum:fl-algorithm-feature-d})は、Federated Learningにおいては最も重要でない
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{algorithm}[H]
	\DontPrintSemicolon
	\caption{SVRG; Stochastic Variance Reduced Gradient (Recall) ~\cite{stochastic_2017}}
	\label{alg:svrg-recall}
	\Parameter{$m = $ number of stochastic steps per epoch, $h = $ stepsize (learning rate)}
	\begin{algorithmic}[1]
		\FOR{$s = 0, 1, \ldots$}
			\STATE{Compute and store full gradient $\nabla f(\bm{w}^t) = \frac{1}{N} \sum_{i = 1}^N \nabla f_i(\bm{w}^t)$} \tcp*{Full pass through data}
			\STATE{Set $\bm{w} = \bm{w}^t$}
			\FOR{$t = 1$ \TO $m$}
				\STATE{Pick $i \in \left\{ 1, \ldots, N \right\}$, uniformly at random}
				\STATE{Update using $\bm{w} = \bm{w} - h \left[ \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right]$} \tcp*{Stochastic update}
			\ENDFOR
			\STATE{$\bm{w}^{t + 1} = \bm{w}$}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item SVRG; Stochastic Variance Reduced Gradient
	\begin{itemize}
		\item 外側のループでは、Full gradient(全データについての勾配の平均)$\nabla f(\bm{w}^t)$を計算
		\item 内側のループでは、確率的勾配によるパラメータの更新を$m$回実行 \\
		$\Rightarrow$ $m$は、データ数$N$の$1$倍から$5$倍程度の値に設定 \\
		$\Rightarrow$ $m$は、実用上は$N$にすることが多い
		\newline
		
		\item 内側のループでは、データ点$i$における勾配$\nabla f_i(\bm{w}), \nabla f_i(\bm{w}^t)$を計算 \newline \newline
		$\Rightarrow$ これらの勾配の差$\nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t)$を求めて、$\bm{w}$と$\bm{w}^t$におけるFull gradientの差分$\nabla f(\bm{w}) - \nabla f(\bm{w}^t)$を推定するための項とする \newline \newline
		$\Rightarrow$ $\nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t)$は、$\nabla f(\bm{w})$の\alert{不偏推定量}を導く
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item SVRG; Stochastic Variance Reduced Gradient
	\begin{itemize}
		\item 更新中の$\bm{w}$と、固定された$\bm{w}^t$との差が小さければ、$\nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t)$も小さくなるので、$\nabla f(\bm{w})$(の予測値)に加わるノイズも小さくなっていると予想できる
		\newline
		
		\item 内側のループを実行する度に、$\bm{w}$が$\bm{w}^t$から離れていくので、$\nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t)$が増大し、従ってFull gradient$\nabla f(\bm{w}^t)$に加わるノイズが増大 \\
		$\Rightarrow$ 外側のループが実行され、新たなFull gradient$\nabla f(\bm{w}^{t + 1})$が計算されると、Full gradientに加わるノイズは再び小さくなる
		\newline
		
		\item 関数$f = \frac{1}{N} \sum_i f_i$が$\lambda$-strongly convex functionで、各$f_i$が$L$-smooth functionならば、収束度合いは次のように表される(詳細は論文を参照)
		\begin{equation}
			\mathbb{E} \left[ f(\bm{w}^t) - f(\bm{w}^*) \right] \le c^t \left[ f(\bm{w}^0) - f(\bm{w}^*) \right]
		\end{equation}
		$\bm{w}^*$は$f(\bm{w})$を最小化する最適解、$c = \Theta \left( \frac{1}{mh} \right) + \Theta(h)$(詳細は略)
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための問題設定}

\begin{itemize}
	\item Federated Learningのための問題設定
	\begin{itemize}
		\item 解くべき問題は、経験損失$f(\bm{w})$の最小化であった
		\begin{equation}
			\min_{\bm{w} \in \mathbb{R}^D} f(\bm{w}), \qquad f(\bm{w}) = \frac{1}{N} \sum_{i = 1}^N f_i(\bm{w})
		\end{equation}
		\item 各$f_i$は凸関数で、訓練データ$\left\{ \bm{x}_i, y_i \right\}_{i = 1}^N$が与えられている(多数のノードに不均等に分散)
	\end{itemize} \
	
	\item 分散学習のために、以下の記法を導入する
	\begin{itemize}
		\item $K$を\alert{ノード数}とする
		\item $\mathcal{P}_k$を、ノード$k \in \left\{ 1, \ldots, K \right\}$が持つ訓練データの\alert{インデックス集合}とする
		\item ノード$k$が持つ訓練データの個数を、$N_k = \left| \mathcal{P}_k \right|$と表す \\
		$\Rightarrow$ $k \neq l$のとき$\mathcal{P}_k \cap \mathcal{P}_l = \varnothing$(空集合)、そして$\sum_{k = 1}^K N_k = N$
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための問題設定}

\begin{itemize}
	\item Federated Learningのための問題設定
	\begin{itemize}
		\item 経験損失$f(\bm{w})$を、次の手順で書き直す
		\begin{eqnarray}
			F_k(\bm{w}) &\equiv& \frac{1}{N_k} \sum_{i \in \mathcal{P}_k} f_i(\bm{w}) \\
			f(\bm{w}) &=& \frac{1}{N} \sum_{i = 1}^N f_i(\bm{w}) = \frac{1}{N} \sum_{k = 1}^K \sum_{i \in \mathcal{P}_k} f_i(\bm{w}) \nonumber \\
			&=& \frac{1}{N} \sum_{k = 1}^K N_k \cdot \frac{1}{N_k} \sum_{i \in \mathcal{P}_k} f_i(\bm{w}) = \frac{1}{N} \sum_{k = 1}^K N_k F_k(\bm{w})
		\end{eqnarray}
		
		\item $F_k(\bm{w})$は、各ノード$k$が最小化すべき目的関数である(凸関数)
		\item これより、解くべき問題は次のように書き直される
		\begin{equation}
			\min_{\bm{w} \in \mathbb{R}^D} f(\bm{w}), \qquad f(\bm{w}) = \sum_{k = 1}^K \frac{N_k}{N} F_k(\bm{w})
		\end{equation}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための問題設定}

\begin{itemize}
	\item Federated Learningのための問題設定
	\begin{itemize}
		\item この問題を解くための最も簡単な手法は、次の通りである
		\begin{equation}
			\bm{w}_k^{t + 1} = \argmin_{\bm{w} \in \mathbb{R}^D} F_k(\bm{w}), \qquad \bm{w}^{t + 1} = \sum_{k = 1}^K \frac{N_k}{N} \bm{w}_k^{t + 1}
		\end{equation}
		
		\item 各ノード上で目的関数$F_k$を最小化し、得られた解$\bm{w}_k^{t + 1}$の$N_k$による重み付け和を$\bm{w}$とする
		\newline
		
		\item この場合、上の問題を一度だけ解けば、解$\bm{w}$が得られる($\bm{w}_k^{t + 1}$の右辺は$t$には依存しない)ので、各デバイスと中央のノードとの一度だけのやりとりで済む
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための問題設定}

\begin{itemize}
	\item Federated Learningのための問題設定
	\begin{itemize}
		\item 上記のアルゴリズムは\alert{動作しない} \\
		$\Leftarrow$ 全体の解$\bm{w}$が、個々の解$\bm{w}_k$の重み付け和になっているとは考えにくい \newline \newline
		$\Leftarrow$ 関数$F_k$の形が、全ての$k$について等しいならば、重み付け和にはなっている \\
		$\Leftarrow$ 関数の形が全て等しいならば、単一のノード上で$\min_{\bm{w} \in \mathbb{R}^D} F_1(\bm{w})$を解けばよいので、分散アルゴリズムを考える必要はない
		\newline
		
		\item 分散アルゴリズムを導出したいが、上記のアルゴリズムでは無意味
		\item 但し、各ノード$k$が、目的関数$F_k$に含まれる曲がり具合の情報(Curvature information)を最大限活用できるようにしたい
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための問題設定}

\begin{itemize}
	\item Federated Learningのための問題設定
	\begin{itemize}
		\item 分散アルゴリズムを導出するために、各$F_k$に二次の項$-\left( \bm{a}_k^t \right)^T \bm{w} + \frac{\mu}{2} || \bm{w} - \bm{w}^t ||^2$を摂動として加算する
		\newline
		
		\item そして、各ノードが次の問題を解くようにする
		\begin{eqnarray}
			\bm{w}_k^{t + 1} &=& \argmax_{\bm{w} \in \mathbb{R}^D} \left( F_k(\bm{w}) - \left( \bm{a}_k^t \right)^T \bm{w} + \frac{\mu}{2} || \bm{w} - \bm{w}^t ||^2 \right) \\
			\bm{w}^{t + 1} &=& \frac{1}{K} \sum_{k = 1}^K \bm{w}_k^{t + 1}
		\end{eqnarray}
		
		\item 各ノード$k$が、目的関数$F_k$に含まれる曲がり具合の情報(Curvature information)を最大限活用できるようにしたい \\
		$\Rightarrow$ 各ノードが最適化する関数の、ヘッセ行列は$\nabla^2 F_k + \mu \bm{I}$となるので、関数$F_k$に含まれる勾配の情報は、殆どそのまま保存される
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための問題設定}

\begin{itemize}
	\item Federated Learningのための問題設定
	\begin{itemize}
		\item 以下の式を解きたいが、ベクトル$\bm{a}_k^t$の決め方が分からない
		\begin{equation}
			\bm{w}_k^{t + 1} = \argmax_{\bm{w} \in \mathbb{R}^D} \left( F_k(\bm{w}) - \left( \bm{a}_k^t \right)^T \bm{w} + \frac{\mu}{2} || \bm{w} - \bm{w}^t ||^2 \right) \nonumber
		\end{equation}
		
		\item $t \to \infty$の極限では、$\bm{w}$が最適($\bm{w} = \bm{w}^*$)であるとき、上式の勾配が$0$となって欲しい
		\begin{eqnarray}
			&& \nabla \left( F_k(\bm{w}) - \left( \bm{a}_k^t \right)^T \bm{w} + \frac{\mu}{2} || \bm{w} - \bm{w}^t ||^2 \right) \nonumber \\
			&=& \nabla F_k(\bm{w}) - \bm{a}_k^t + \mu \left( \bm{w} - \bm{w}^t \right) = 0 \nonumber
		\end{eqnarray}
		
		\item 即ち、$t \to \infty$の極限では、$\bm{a}_k^t$は次のようになって欲しい
		\begin{equation}
			\bm{a}_k^t = \nabla F_k(\bm{w}) + \mu \left( \bm{w} - \bm{w}^t \right) \simeq \nabla F_k(\bm{w}^*) \ \left( \because \bm{w}^* \simeq \bm{w}^t \right) \nonumber
		\end{equation}
		
		\item 但し、$\bm{w}^*$を知らないので、$\bm{a}_k^t = \nabla F_k(\bm{w}^*)$とはできない \\
		$\Rightarrow$ $t \to \infty$で、$\bm{a}_k^t \to \nabla F_k(\bm{w}^*)$となるような更新式を編み出す
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item DANE; Distributed Approximate Newton
	\begin{itemize}
		\item 先程の最適化問題は、双対問題と深く関連している \\
		$\Rightarrow$ 但し、上記のような問題がノード数分だけ存在するので、複雑である
		\newline
		
		\item DANEアルゴリズムでは、個々のノード上で解くための部分問題を構成することに主眼を置く \\
		$\Leftarrow$ 部分問題は、各ノード上のデータと、完全な勾配$\nabla f(\bm{w}^t)$にのみ依存 \\
		$\Leftarrow$ Full gradient$\nabla f(\bm{w}^t)$は、各デバイスと中央のノード間で、1度だけやり取りすれば計算可能
		\newline
		
		\item DANEアルゴリズムを次に示す
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{algorithm}[H]
	\DontPrintSemicolon
	\caption{DANE; Distributed Approximate Newton ~\cite{1610.02527}}
	\label{alg:dane}
	\Input{regularizer $\mu \ge 0$, parameter $\eta$ (default: $\mu = 0, \eta = 1$)}
	\begin{algorithmic}[1]
		\STATE{Initialize $\bm{w}^0$}
		\FOR{$t = 0, 1, \ldots$}
			\STATE{Compute $\nabla f(\bm{w}^t) = \frac{1}{N} \sum_{i = 1}^N \nabla f_i(\bm{w}^t)$}
			\STATE{Distribute $\nabla f(\bm{w}^t)$ to all machines}
			\STATE{For each node $k \in \left\{ 1, \ldots, K \right\}$, solve
			\begin{eqnarray}
				&& \bm{w}_k^{t + 1} = \argmin_{\bm{w} \in \mathbb{R}^D} \bigg( F_k(\bm{w}) - \left( \nabla F_k(\bm{w}^t) - \eta \nabla f(\bm{w}^t) \right)^T \bm{w} \nonumber \\
				&& \qquad \qquad \qquad + \frac{\mu}{2} || \bm{w} - \bm{w}^t ||^2 \bigg)
			\end{eqnarray}}
			\STATE{Compute $\bm{w}^{t + 1} = \frac{1}{K} \sum_{k = 1}^K \bm{w}_k^{t + 1}$}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item DANE; Distributed Approximate Newton
	\begin{itemize}
		\item 5行目では、各ノードが次の部分問題を解いている
		\begin{equation}
			\bm{w}_k^{t + 1} = \argmin_{\bm{w} \in \mathbb{R}^D} \left( F_k(\bm{w}) - \left( \nabla F_k(\bm{w}^t) - \eta \nabla f(\bm{w}^t) \right)^T \bm{w} + \frac{\mu}{2} || \bm{w} - \bm{w}^t ||^2 \right) \nonumber
		\end{equation}
		
		\item この部分問題の解を得るためのアルゴリズムは、特に指定されていない(何でもよい)
		\item 問題を解くうえで、他のノードと通信する必要がない(通信コストを十分に小さくできる)
		\newline
		
		\item 各ノードが、摂動を加えた最適化問題を解くアルゴリズムの1つ \\
		$\Leftarrow$ $\bm{a}_k^t$を、$\bm{a}_k^t = \nabla F_k(\bm{w}^t) - \eta \nabla f(\bm{w}^t)$と定義している \\
		$\Leftarrow$ $\bm{w}^t \to \bm{w}^*$ならば$\nabla f(\bm{w}^t) \to \nabla f(\bm{w}^*) = 0$であるので、$\bm{a}_k^t \to \nabla F_k(\bm{w}^*)$が成立
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item DANE; Distributed Approximate Newton
	\begin{itemize}
		\item このアルゴリズムは、Federated Learningのために必要な条件(\ref{enum:fl-algorithm-feature-b})と(\ref{enum:fl-algorithm-feature-c})を満たさない
		\item $\mu = 0, \eta = 1$ならば、条件(\ref{enum:fl-algorithm-feature-d})を満たす
		\item 任意の$\mu, \eta$について、条件(\ref{enum:fl-algorithm-feature-a})を満たす
		\newline
		
		\item このアルゴリズムでは、関数が2回微分可能であること、各ノードが独立同分布な標本を得られることを仮定
		\newline
		
		\item 正則化パラメータ$\mu$の決め方については、改善の余地がある \\
		$\Leftarrow$ $\mu = 0$であれば、ノード数$K$が小さいときは速やかに収束するが、$K$が増えるにつれて、急速に発散しやすくなる \\
		$\Leftarrow$ $\mu$を大きくすれば、アルゴリズムは安定する(発散しづらくなる)が、その分パラメータの収束は遅くなる
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item DANEとSVRGを融合したアルゴリズム
	\begin{itemize}
		\item DANEアルゴリズムは、Federated Learningに適用できない(必要な条件を満たさない)
		\item 部分問題の最適解を得る必要がある \\
		$\Leftarrow$ 簡単な問題なら可能だが、複雑な問題であれば計算コストが掛かり過ぎる \\
		$\Rightarrow$ 完全な最適解を得るのではなく、近似解を得るようなアルゴリズムに置き換える \\
		$\Rightarrow$ 部分問題を解くアルゴリズムとして、先程の\alert{SVRG}を使用
		\newline
		
		\item SVRGアルゴリズムでは、外側のループの最初で、完全な勾配$\nabla f(\bm{w}^t)$を計算する必要があった(2行目)
		\item 完全な勾配$\nabla f(\bm{w}^t)$は、各ノードが部分問題を解く前の段階で、既に求まっている(3行目) \\
		$\Rightarrow$ 各ノードでは、SVRGアルゴリズムの内側のループのみが実行され(後述)、完全な勾配を求める必要はない(既知であるとして扱う)
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{algorithm}[H]
	\DontPrintSemicolon
	\caption{SVRG; Stochastic Variance Reduced Gradient (Recall) ~\cite{stochastic_2017}}
	\label{alg:svrg-recall-2}
	\Parameter{$m = $ number of stochastic steps per epoch, $h = $ stepsize (learning rate)}
	\begin{algorithmic}[1]
		\FOR{$s = 0, 1, \ldots$}
			\STATE{Compute and store full gradient $\nabla f(\bm{w}^t) = \frac{1}{N} \sum_{i = 1}^N \nabla f_i(\bm{w}^t)$} \tcp*{Full pass through data}
			\STATE{Set $\bm{w} = \bm{w}^t$}
			\FOR{$t = 1$ \TO $m$}
				\STATE{Pick $i \in \left\{ 1, \ldots, N \right\}$, uniformly at random}
				\STATE{Update using $\bm{w} = \bm{w} - h \left[ \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right]$} \tcp*{Stochastic update}
			\ENDFOR
			\STATE{$\bm{w}^{t + 1} = \bm{w}$}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{algorithm}[H]
	\DontPrintSemicolon
	\caption{DANE; Distributed Approximate Newton (Recall) ~\cite{1610.02527}}
	\label{alg:dane-recall}
	\Input{regularizer $\mu \ge 0$, parameter $\eta$ (default: $\mu = 0, \eta = 1$)}
	\begin{algorithmic}[1]
		\STATE{Initialize $\bm{w}^0$}
		\FOR{$t = 0, 1, \ldots$}
			\STATE{Compute $\nabla f(\bm{w}^t) = \frac{1}{N} \sum_{i = 1}^N \nabla f_i(\bm{w}^t)$}
			\STATE{Distribute $\nabla f(\bm{w}^t)$ to all machines}
			\STATE{For each node $k \in \left\{ 1, \ldots, K \right\}$, solve
			\begin{eqnarray}
				&& \bm{w}_k^{t + 1} = \argmin_{\bm{w} \in \mathbb{R}^D} \bigg( F_k(\bm{w}) - \left( \nabla F_k(\bm{w}^t) - \eta \nabla f(\bm{w}^t) \right)^T \bm{w} \nonumber \\
				&& \qquad \qquad \qquad + \frac{\mu}{2} || \bm{w} - \bm{w}^t ||^2 \bigg)
			\end{eqnarray}}
			\STATE{Compute $\bm{w}^{t + 1} = \frac{1}{K} \sum_{k = 1}^K \bm{w}_k^{t + 1}$}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item DANEとSVRGとの関係性
	\begin{itemize}
		\item DANEアルゴリズムは、ある特定の条件下では、分散化されたバージョンのSVRGアルゴリズムと等価
		\newline
		
		\item 以下の2つのアルゴリズムは等価である
		\item 下記の2つは、同一のパラメータ列$\left\{ \bm{w}^t \right\}$を出力
	\end{itemize}
	
	\begin{enumerate}
		\item $\mu = 0, \eta = 1$の下でDANEを実行し、その部分問題はSVRGアルゴリズムで解く
		\item 分散化されたバージョンのSVRGアルゴリズムを解く(後述)
	\end{enumerate}
	
	\begin{itemize}
		\item 分散化されたSVRGアルゴリズム(Naive Federated SVRG)を次に示す
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{algorithm}[H]
	\DontPrintSemicolon
	\caption{Naive Federated SVRG (FSVRG) ~\cite{1610.02527}}
	\label{alg:naive-federated-svrg}
	\Parameter{$m = $number of stochastic steps per epoch, $h = $stepsize, data partition $\left\{ \mathcal{P}_k \right\}_{k = 1}^K$} 
	\begin{algorithmic}[1]
		\STATE{Initialize $\bm{w}^0$}
		\FOR{$t = 0, 1, \ldots, $}
			\STATE{Compute $\nabla f(\bm{w}^t) = \frac{1}{N} \sum_{i = 1}^N \nabla f_i(\bm{w}^t)$, distribute to all machines}
			\FOR{$k = 1$ to $K$ \textbf{do in parallel} over nodes $k$}
				\STATE{Initialize $\bm{w}_k = \bm{w}^t$}
				\FOR{$s = 1$ to $m$}
					\STATE{Sample $i \in \mathcal{P}_k$ uniformly at random}
					\STATE{Update using $\bm{w}_k = \bm{w}_k - h \left[ \nabla f_i(\bm{w}_k) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right]$}
				\ENDFOR
			\ENDFOR
			\STATE{Update using $\bm{w}^{t + 1} = \bm{w}^t + \frac{1}{K} \sum_{k = 1}^K \left( \bm{w}_k - \bm{w}^t \right)$}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item 2つのアルゴリズムが等価であることの簡潔な証明
	\begin{itemize}
		\item SVRGアルゴリズムでは、$\nabla f(\bm{w}) = \frac{1}{N} \sum_{i = 1}^N \nabla f_i(\bm{w})$の不偏推定量を得るために、$\nabla f(\bm{w}^t) + \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t)$という項を用いた
		\newline
		\item $\nabla f(\bm{w}^t)$は、外側のループの最初で計算される(適当な時間間隔で計算され、暫くの間固定される)
		\item 内側のループを実行する度に、データ$i \in \left\{ 1, \ldots, N \right\}$について$\nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t)$を計算し、$\nabla f(\bm{w}^t)$を補正する
		\newline
		
		\item DANEにおける部分問題を、各ノードがSVRGで解くことを考える(SVRGの\alert{内側のループのみ}が実行される)
		\item SVRGで解くのは次の問題である($\mu = 0, \eta = 1$)
		\begin{eqnarray}
			&& \min_{\bm{w} \in \mathbb{R}^D} \left( F_k(\bm{w}) - \left( \nabla F_k(\bm{w}^t) - \eta \nabla f(\bm{w}^t) \right)^T \bm{w} + \frac{\mu}{2} || \bm{w} - \bm{w}^t ||^2 \right) \nonumber \\
			&=& \min_{\bm{w} \in \mathbb{R}^D} \left( F_k(\bm{w}) - \left( \nabla F_k(\bm{w}^t) - \nabla f(\bm{w}^t) \right)^T \bm{w} \right)
		\end{eqnarray}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item 2つのアルゴリズムが等価であることの簡潔な証明
	\begin{itemize}
		\item SVRGで最小化しようとしている関数の勾配を求める
		\begin{eqnarray}
			&& \frac{\partial}{\partial \bm{w}} \left( F_k(\bm{w}) - \left( \nabla F_k(\bm{w}^t) - \nabla f(\bm{w}^t) \right)^T \bm{w} \right) \nonumber \\
			&=& \nabla F_k(\bm{w}) - \nabla F_k(\bm{w}^t) - \nabla f(\bm{w}^t)
		\end{eqnarray}
		
		\item 上式について、$\nabla f(\bm{w}^t)$は、DANEアルゴリズムの3行目で既に求まっている
		\item 残りの$\nabla F_k(\bm{w})$と$\nabla F_k(\bm{w}^t)$の計算について考える
		\newline
		
		\item $\nabla F_k(\bm{w}) = \sum_{i \in \mathcal{P}_k} \nabla f_i(\bm{w})$であるから、$\nabla F_k(\bm{w})$は、$i \in \mathcal{P}_k$である全ての$i$についての、勾配$\nabla f_i(\bm{w})$の足し合わせである($\nabla F_k(\bm{w}^t)$についても同様)
		\newline
		
		\item SVRGの内側のループでは、ある1つのインデックス$i \in \mathcal{P}_k$をランダムに選び出し、パラメータ$\bm{w}$を更新する
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item 2つのアルゴリズムが等価であることの簡潔な証明
	\begin{itemize}
		\item SVRGの内側のループでは、ある1つのインデックス$i \in \mathcal{P}_k$をランダムに選び出し、パラメータ$\bm{w}$を更新する \newline \newline
		$\Rightarrow$ 内側のループでは、1つのデータ点$i$についての\alert{確率的勾配だけを使って}、$\nabla F_k(\bm{w}) - \nabla F_k(\bm{w}^t) - \nabla f(\bm{w}^t)$を推定したい \newline \newline
		$\Rightarrow$ $\nabla f(\bm{w}^t)$は既知であるから、残りの項$\nabla F_k(\bm{w}) - \nabla F_k(\bm{w}^t)$を、確率的勾配$\nabla f_i(\bm{w}), \nabla f_i(\bm{w}^t)$を使って近似する
		\newline
		
		\item SVRGのときと同じ方法で、次のように近似できる
		\begin{eqnarray}
			\nabla F_k(\color{red}\bm{w}\normalcolor) &=& \nabla F_k(\bm{w}^t) + \left( \nabla f_i(\color{red}\bm{w}\normalcolor) - \nabla f_i(\bm{w}^t) \right) \\
			\nabla F_k(\color{red}\bm{w}^t\normalcolor) &=& \nabla F_k(\bm{w}^t) + \left( \nabla f_i(\color{red}\bm{w}^t\normalcolor) - \nabla f_i(\bm{w}^t) \right)
		\end{eqnarray}
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item 2つのアルゴリズムが等価であることの簡潔な証明
	\begin{itemize}
		\item 以下は単に、$f(\bm{w})$を$F_k(\bm{w})$に置き換えているだけ
		\begin{equation}
			\nabla F_k(\bm{w}) = \nabla F_k(\bm{w}^t) + \left( \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) \right)
		\end{equation}
		
		\item $\nabla F_k(\bm{w}^t)$を、内側のループが実行される度に計算するのは、計算コストの観点から避けたいので、適当な時間間隔で求めて、暫くの間は使い回すことにする \newline \newline
		$\Rightarrow$ 現在のパラメータ$\bm{w}$についての勾配$\nabla F_k(\bm{w})$の不偏推定量を得るために、古いパラメータ$\bm{w}^t$についての勾配$\nabla F_k(\bm{w}^t)$を、\color{red}データ$i$についての項$\nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t)$で補正\normalcolor する
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item 2つのアルゴリズムが等価であることの簡潔な証明
	\begin{itemize}
		\item このような近似の下で、内側のループで計算される勾配は、次のようになる
		\begin{eqnarray}
			&& \nabla F_k(\bm{w}) - \nabla F_k(\bm{w}^t) - \nabla f(\bm{w}^t) \\
			&=& \left[ \nabla F_k(\bm{w}^t) + \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) \right] \nonumber \\
			&& \qquad - \left[ \nabla F_k(\bm{w}^t) + \nabla f_i(\bm{w}^t) - \nabla f_i(\bm{w}^t) \right] - \nabla f(\bm{w}^t) \\
			&=& \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t)
		\end{eqnarray}
		
		\item これより、DANEアルゴリズムの部分問題に、SVRGアルゴリズムを適用したとき、各ノードにおけるパラメータの更新式は次のようになる
		\begin{equation}
			\bm{w} = \bm{w} - h \left[ \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right]
		\end{equation}
		
		\item 上の更新式は、FSVRG(Algorithm \ref{alg:naive-federated-svrg})の8行目と等しい \\
		$\Rightarrow$ DANE($\mu = 0, \eta = 1$)の部分問題にSVRGを組み込んだアルゴリズムは、(Naiveな)FSVRGと等価
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item Naive Federated SVRGについての補足
	\begin{itemize}
		\item 外側のループについて、最後の更新式が次のようになっている
		\begin{equation}
			\bm{w}^{t + 1} = \bm{w}^t + \frac{1}{K} \sum_{k = 1}^K \left( \bm{w}_k - \bm{w}^t \right)
		\end{equation}
		
		\item これは、DANEにおける次の式に等しい
		\begin{equation}
			\bm{w}^{t + 1} = \frac{1}{K} \sum_{k = 1}^K \bm{w}_k^{t + 1}
		\end{equation}
		
		\item $\left( \bm{w}_k - \bm{w}^t \right)$は、各ノードから中央のサーバに送られる、パラメータの差分を表している \\
		$\Rightarrow$ $\bm{w}^t$が中央のサーバから送られてきたパラメータ、$\bm{w}_k$はノード$k$上の訓練データを使って更新されたパラメータ \\
		$\Rightarrow$ $\bm{w}_k$をそのまま送っても良いが、差分を送った方がデータを圧縮できる(通信量を削減)
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item SVRGアルゴリズムの導出
	\begin{itemize}
		\item (Naiveな)Federated SVRGを更に改良したアルゴリズムを、ここでは提案する \\
		$\Leftarrow$ 各デバイスに保存された訓練データの個数、訓練データのスパース性、データが独立同分布でない(\alert{Non-IID})ことについて考慮
		\newline
		
		\item 各デバイス間では、訓練データの個数が大きく異なる
		\item データのある特徴は、ごく少数のノードにだけ出現し、それ以外の大多数のノードには出現しないかもしれない
		\item 各デバイス上のデータは、データ全体の分布を表していない(データ全体は、デバイスの地理的な位置やタイムゾーンなどが影響し、特定のパターンによってクラスタ化されているかもしれない)
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item SVRGアルゴリズムの導出(記法の整理)
	\begin{itemize}
		\item $N$: 訓練データの個数, $K$: ノード数, $D$: パラメータの次元
		\newline
		
		\item $\mathcal{P}_k$: ノード$k$が保持する訓練データのインデックスの集合 \\
		$\Rightarrow$ $\mathcal{P}_k \subseteq \left\{ 1, \ldots, N \right\}, \forall k \neq l \ \mathcal{P}_k \cap \mathcal{P}_l = \varnothing$ \\
		\item $N_k = |\mathcal{P}_k|$: ノード$k$が保持する訓練データの個数 \\
		$\Rightarrow$ $\sum_{k = 1}^K N_k = N$
		\newline
		
		\item $N^j = \left| \left\{ i \in \left\{ 1, \ldots, N \right\} | \bm{x}_i^T \bm{e}_j \neq 0 \right\} \right|$: $j$番目の次元が$0$ではない訓練データの個数
		\item $N_k^j = \left| \left\{ i \in \mathcal{P}_k | \bm{x}_i^T \bm{e}_j \neq 0 \right\} \right|$: ノード$k$が保持する、$j$番目の次元が$0$ではない、訓練データの個数
		\newline
		
		\item $\phi^j = N^j / N$: 全訓練データのうち、$j$番目の次元が$0$でないものの割合
		\item $\phi_k^j = N_k^j / N_k$: ノード$k$が保持する全訓練データのうち、$j$番目の次元が$0$でないものの割合
		
		\framebreak
		
		\item $s_k^j = \phi^j / \phi_k^j$: 全訓練データと、ノード$k$が保持する訓練データにおける、$j$番目の次元が$0$でない割合の比率 \\
		$\Rightarrow$ $s_k^j$が小さいとき、ノード$k$は、次元$j$に関する訓練データを、他のノードよりも多く持っていることを意味している \\
		$\Rightarrow$ $s_k^j$の逆数は、ノード$k$が持っている訓練データは、次元$j$に関してどの程度レアかを表すと考えられる
		\newline
		
		\item $\bm{S}_k = \diag \left( s_k^j \right)$: $s_k^j$を並べた対角行列($\bm{S}_k \in \mathbb{R}^{D \times D}$)
		\item $\omega^j = \left| \left\{ \mathcal{P}_k | n_k^j \neq 0 \right\} \right|$: 次元$j$が$0$ではない訓練データを持っている、ノードの数
		\newline
		
		\item $a^j = K / \omega^j$: 次元$j$が$0$でない訓練データが、ノードに出現する割合 \\
		$\Rightarrow$ 次元$j$のレア度を表す
		\item $\bm{A} = \diag \left( a^j \right)$: $a^j$を並べた対角行列($\bm{A} \in \mathbb{R}^{D \times D}$)
		\newline
		
		\item 提案されたFederated SVRGアルゴリズムを、次に示す
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{algorithm}[H]
	\DontPrintSemicolon
	\caption{Federated SVRG (FSVRG) ~\cite{1610.02527}}
	\label{alg:federated-svrg}
	\Parameter{$h = $stepsize, data partition $\left\{ \mathcal{P}_k \right\}_{k = 1}^K$, diagonal matrix $\bm{A}, \bm{S}_k \in \mathbb{R}^{D \times D}$ for $k \in \left\{ 1, \ldots, K \right\}$} 
	\begin{algorithmic}[1]
		\FOR{$t = 0, 1, \ldots$}
			\STATE{Compute $\nabla f(\bm{w}^t) = \frac{1}{N} \sum_{i = 1}^N \nabla f_i(\bm{w}^t)$}
			\FOR{$k = 1$ to $K$ \textbf{do in parallel} over nodes $k$}
				\STATE{Initialize $\bm{w}_k = \bm{w}^t$, $h_k = h / N_k$}
				\STATE{Let $\left\{ i_s \right\}_{s = 1}^{N_k}$ be random permutation of $\mathcal{P}_k$}
				\FOR{$s = 1, \ldots, N_k$}
					\STATE{$\bm{w}_k = \bm{w}_k - \color{red}h_k\normalcolor \left[ \color{red}\bm{S}_k\normalcolor \left[ \nabla f_{i_s}(\bm{w}_k) - \nabla f_{i_s}(\bm{w}^t) \right] + \nabla f(\bm{w}^t) \right]$}
				\ENDFOR
			\ENDFOR
			\STATE{$\bm{w}^t = \bm{w}^t + \color{red}\bm{A}\normalcolor \sum_{k = 1}^K \color{red}\frac{N_k}{N}\normalcolor \left( \bm{w}_k - \bm{w}^t \right)$}
		\ENDFOR
	\end{algorithmic}
\end{algorithm}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{itemize}
	\item Federated SVRGアルゴリズムの改良点
	\begin{enumerate}
		\item ノードごとにステップサイズを変更: $h_k = h / N_k$
		\item 各ノードが保持するデータ数に比例した更新量: $\frac{N_k}{N} \left( \bm{w}_k - \bm{w}^t \right)$
		\item 確率的勾配の各次元に対するスケーリング: $\bm{S}_k$
		\item パラメータの差分の各次元に対するスケーリング: $\bm{A} \left( \bm{w}_k - \bm{w}^t \right)$
	\end{enumerate}
	
	\begin{itemize}
		\item \sout{個人的に、このようなアルゴリズムの細工にはあまり魅力を感じない}
		\item \sout{このような細工よりも、単純にデータ数を増やせば良いんじゃないか?}
		\newline
		
		\item このアルゴリズムは、他のアルゴリズムよりも圧倒的に収束が速い
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Federated Learningのための最適化アルゴリズム}

\begin{enumerate}
	\item ノードごとにステップサイズを変更: $h_k = h / N_k$
	\begin{itemize}
		\item Naive Federated SVRGでは、各ノードでのパラメータの更新回数は、$m$回に統一されていた \\
		$\Rightarrow$ しかし、各ノードが保持する訓練データの数は大きく異なるのだから、パラメータの更新回数を、全ノードにわたって同じにするのは良くない \\
		$\Rightarrow$ それゆえ、各ノードでは、自身が持つ全ての訓練データを使って、パラメータを更新
		\newline
		
		\item ステップサイズを$N_k$に反比例するように設定し、各ノードのパラメータ更新量が同程度の大きさになるようにする
	\end{itemize}
	
	\framebreak
	
	\item 各ノードが保持するデータ数に比例した更新量: $\frac{N_k}{N} \left( \bm{w}_k - \bm{w}^t \right)$
	\begin{itemize}
		\item Naive Federated SVRGでは、以下のようにパラメータ$\bm{w}$が更新された
		\begin{eqnarray}
			\bm{w}^{t + 1} &=& \bm{w}^t + \frac{1}{K} \sum_{k = 1}^K \left( \bm{w}_k - \bm{w}^t \right) \nonumber \\
			&=& \bm{w}^t + \frac{h}{K} \sum_{k = 1}^K \sum_i \left[ \nabla f_i(\bm{w}_k) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right] \nonumber \\
			&=& \bm{w}^t + \frac{h}{K} \sum_{k = 1}^K \sum_i \left[ \nabla f_i(\color{red}\bm{w}\normalcolor) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right] \nonumber \\
			&\simeq& \bm{w}^t + \frac{h}{K} \sum_{k = 1}^K \mathbb{E} \left[ \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right] \nonumber \\
			&=& \bm{w}^t + \frac{h}{K} \mathbb{E} \left[ \sum_{k = 1}^K \left( \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right) \right] \nonumber
		\end{eqnarray}
		
		\framebreak
		
		\item 上式における$i$は、$\mathcal{P}_k$から重複を許して、適当に選択された$m$個のインデックスであるとする (但し、最後の式における$i$は、$\mathcal{P}_k$から適当に選択されたインデックスである)
		\item また、ある時点において、全ての$k \in \left\{ 1, \ldots, K \right\}$について$\bm{w}_k = \bm{w}$であるとしている
		\newline
		
		\item ここでは、適当な$\alpha_k$を導入して、$\nabla f(\bm{w}^t)$の不偏推定量が得られるようにしたい($i$は$\mathcal{P}_k$から適当に選択されたもの) \\
		$\Leftarrow$ 勾配降下法などの、目的関数の勾配のみを用いたアルゴリズム(\alert{一次法}; First-order method)に対して求められる性質
		\newline
		
		\item 即ち、以下が成り立って欲しい($\bm{w}^t$に足される量が、$\nabla f(\bm{w}^t)$に比例する)
		\begin{equation}
			\mathbb{E} \left[ \sum_{k = 1}^K \alpha_k \left( \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right) \right] = \nabla f(\bm{w}^t)
		\end{equation}
		
		\framebreak
		
		\item $\alpha_k$は次のようにして求められる
		\begin{eqnarray}
			&& \mathbb{E} \left[ \sum_{k = 1}^K \alpha_k \left( \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right) \right] \nonumber \\
			&=& \sum_{k = 1}^K \alpha_k \mathbb{E} \left[ \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right] \nonumber \\
			&\simeq& \sum_{k = 1}^K \alpha_k \frac{1}{N_k} \sum_{i \in \mathcal{P}_k} \left[ \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right]
		\end{eqnarray}
		
		\item $\alpha_k = N_k / N$とすることで以下を得る
		\begin{equation}
			= \frac{1}{N} \sum_{k = 1}^K \sum_{i \in \mathcal{P}_k} \left[ \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right] \simeq \nabla f(\bm{w}) \nonumber
		\end{equation}
		
		\framebreak
		
		\item これより、各ノードで計算されたパラメータの差分が、$N_k$に比例するように、更新量を調節することが考えられる
		\begin{eqnarray}
			\bm{w}^{t + 1} &=& \bm{w}^t + \frac{h}{K} \mathbb{E} \left[ \sum_{k = 1}^K \frac{N_k}{N} \left( \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right) \right] \nonumber \\
			&=& \bm{w}^t + \frac{h}{K} \frac{N_k}{N} \mathbb{E} \left[ \sum_{k = 1}^K \left( \nabla f_i(\bm{w}) - \nabla f_i(\bm{w}^t) + \nabla f(\bm{w}^t) \right) \right] \nonumber
		\end{eqnarray}
	\end{itemize}
	
	\framebreak
	
	\item 確率的勾配の各次元に対するスケーリング: $\bm{S}_k$
	\begin{itemize}
		\item 行列$\bm{S}_k$は、$s_k^j = \phi^j / \phi_k^j$を対角成分にもつ
		\newline
		
		\item データ数が$N = 10^6$、ノード数が$K = 10^3$であるとする
		\item 次元$j$が非零になるデータは$10^3$個あるが、その全てが、ある1つのノード$k$に集中しているとする
		\item このとき$s_k^j = \phi^j / \phi_k^j = 10^{-3}$であるので、ノード$k$でパラメータ$\bm{w}$が更新されるとき、次元$j$の更新量は$10^3$分の$1$倍される
		\newline
		
		\item 行列$\bm{S}_k$でスケーリングを行わない場合、ノード$k$における、次元$j$に対する更新量は、他のノードの$10^3$倍程度になる(或いは、パラメータ$\bm{w}$の次元$j$は、ノード$k$でしか更新されない) \\
		$\Rightarrow$ パラメータ$\bm{w}$の次元$j$について、急速に発散してしまう恐れがある
		\newline
		
		\item パラメータ$\bm{w}$の更新に使用する、勾配の(推定量の)各次元が、大体同じになるように揃える役割(個人的には、まだよく理解できていない)
	\end{itemize}
	
	\framebreak
	
	\item パラメータの差分の各次元に対するスケーリング: $\bm{A} \left( \bm{w}_k - \bm{w}^t \right)$
	\begin{itemize}
		\item 行列$\bm{A}$は、$a_j = K / \omega_j$を対角成分にもつ
		\newline
		
		\item 行列$\bm{A}$の各成分は、$\omega_j$に反比例している \\
		$\Rightarrow$ 次元$j$が非零であるデータを持つノード数$\omega_j$が少ないほど、パラメータ$\bm{w}$の次元$j$に対する更新量は大きくなる \newline \newline
		$\Rightarrow$ $\omega_j$が小さいとき、少ないノードから、次元$j$に関する情報が伝達されているので、その情報の価値は高い(パラメータ$\bm{w}$の次元$j$の更新に際して、大いに役立つ情報である)と考えられる(従って更新量を大きくする)
		\newline
		
		\item 4つの改善のうち、どれが最も効果的であったかは不明であるほか、これらの改善策が、相互に干渉し合うと思われる
		\item $h_k, \frac{N_k}{N}, \bm{S}_k, \bm{A}$によって、適切なスケーリングを行うことで、各ノード上の訓練データ数のばらつき、データのスパース性などに対処できそうなのは、直感的には分かる
	\end{itemize}
\end{enumerate}

\end{frame}

\section{最適化アルゴリズムの比較}

\begin{frame}{最適化アルゴリズムの比較}

\begin{itemize}
	\item 最適化アルゴリズムの比較
	\begin{itemize}
		\item Google+の投稿に、1つ以上のコメントが付くかどうかを予測する、二値分類のタスク
		\item L2正則化項を含めたロジスティック回帰
		\newline
		
		\item Google+の投稿から作成したデータセットを、実験に使用
		\item 各ユーザの投稿が、各ユーザの使うデバイス上に保存されているという状況を想定
		\item $10^2$以上のパブリックな投稿を英語で行っている、$10^4$のユーザをランダムに選択
		\item 選択されたユーザの投稿を集めて、そのうちの$75\%$にあたる$N = 2,166,693$個を訓練データとした
		\newline
		
		\item 入力データとなる投稿はBag-of-words形式に変換された \\
		$\Leftarrow$ (全投稿データを基に得られた)最頻出単語$20,000$語と、それ以外の不明な単語の出現回数と、バイアス項を加えたことで、パラメータ数は$D = 20,002$となった
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{最適化アルゴリズムの比較}

\begin{itemize}
	\item 最適化アルゴリズムの比較
	\begin{itemize}
		\item 各ユーザが投稿する内容は一般に異なるので、各ユーザが保持するデータの特徴も全く異なる \\
		$\Rightarrow$ 各ユーザの投稿内容は、全データの分布内で、クラスタを形成していると考えられる \\
		$\Rightarrow$ 各ユーザの保持するデータは、独立同分布標本とは\alert{仮定できない} \\
		$\Rightarrow$ もしそのような仮定をすると、全てのユーザが、あらゆる内容の投稿を満遍なくしていることになる
		\newline
		
		\item 入力データはBag-of-words形式であるから、非常にスパースである \\
		$\Rightarrow$ 殆どの投稿は、ごく一部の単語しか含んでいないので、入力データの中で非零になる要素数が少ない \\
		$\Rightarrow$ 殆どの次元は、非零になる(その次元に対応する特徴が出現する)確率が低い \\
		$\Rightarrow$ 入力データの各特徴の出現確率を、次の図\ref{fig:feature-appearance-on-nodes}に示す(不明な単語を表す次元の出現率が高くなっているが、これは実際とは異なる)
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{最適化アルゴリズムの比較}

\begin{figure}
	\centering
	\includegraphics[keepaspectratio, scale=0.67, clip, trim=2.4cm 17.5cm 2.4cm 2.4cm, page=24]{papers/Federated-Optimization-Distributed-Machine-Learning-for-On-Device-Intelligence-1610-02527.pdf}
	\caption{特徴の出現確率のヒストグラム~\cite{1610.02527}}
	\label{fig:feature-appearance-on-nodes}
\end{figure}

\end{frame}

\begin{frame}{最適化アルゴリズムの比較}

\begin{itemize}
	\item 最適化アルゴリズムの比較
	\begin{itemize}
		\item Google+の投稿に、1つ以上のコメントが付くかどうかを予測する、二値分類のタスク
		\item L2正則化項を含めたロジスティック回帰の、テストデータに対する誤差(分類誤り)を示す
		\newline
		
		\item 全ての投稿に対して、コメントが付かない($-1$)という予測をすると、誤差は$33.16\%$
		\item 入力データを全て用いて、通常のロジスティック回帰を行うと、誤差は$26.27\%$
		\item 各ユーザの全ての投稿に対して、同一の予測をすると、誤差は$17.14\%$ \\
		$\Rightarrow$ 個々の投稿内容ではなく、誰が投稿しているのかに着目して、コメントが付くかどうかの予測を行った方が、精度が良くなることを示唆(直感的にもそうである)
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{最適化アルゴリズムの比較}

\begin{itemize}
	\item 最適化アルゴリズムの比較
	\begin{itemize}
		\item 全ユーザで共通のモデルを基に、各ユーザごとにチューニングすることで、精度を向上させられる可能性 \\
		$\Rightarrow$ もし精度が大幅に向上するのであれば、各ノードごとに、異なるデータのパターンが出現していることも確認できる
		\newline
		
		\item 各最適化アルゴリズムでの性能比較を行う
		\item 提案手法(Federated SVRG; Algorithm \ref{alg:federated-svrg})のハイパーパラメータは、ステップサイズ$h$のみであるが、性能が最も良くなるものを選んだ
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{最適化アルゴリズムの比較}

\begin{itemize}
	\item 最適化アルゴリズムの比較
	\begin{itemize}		
		\item オフラインアルゴリズムで得られる最大の性能(OPT)、勾配降下法(GD)、CoCoA+アルゴリズム(詳細は略)、Federated SVRGアルゴリズム(FSVRG)、ランダムにデータをシャッフルした上で行うFederated SVRG(FSVRGR)の性能を比較
		\begin{itemize}
			\item GD: 個々のノードが、自身が持つ訓練データを全て使って、勾配降下法を行っている
			\item CoCoA+: このスライドでは省略
			\item FSVRG: この論文における提案手法
			\item FSVRGR: 各ノードが保持するデータ数は変化させない(ばらつきは維持する)が、データを一旦全て集めて、ランダムにシャッフルした上で、個々のノードに戻している(各ノードが保持するデータがIIDではなくても、FSVRGアルゴリズムが動作することを示すために用意してある)
		\end{itemize} \
		
		\item 目的関数の推移、またはテストデータに対する分類誤差の推移を縦軸に、そして各デバイスと中央のサーバとの一連のやり取り(Round)の回数を横軸に取ったグラフを、次の図\ref{fig:algorithm-comparison}に示す
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{最適化アルゴリズムの比較}

\begin{figure}
	\centering
	\includegraphics[keepaspectratio, scale=0.67, clip, trim=2.4cm 18cm 2.4cm 2.4cm, page=25]{papers/Federated-Optimization-Distributed-Machine-Learning-for-On-Device-Intelligence-1610-02527.pdf}
	\caption{最適化アルゴリズムの性能比較~\cite{1610.02527}}
	\label{fig:algorithm-comparison}
\end{figure}

\end{frame}

\begin{frame}{最適化アルゴリズムの比較}

\begin{itemize}
	\item 最適化アルゴリズムの比較
	\begin{itemize}
		\item FSVRGは、僅か30回程度のRoundで収束しており、Federated Optimizationにおける課題を最初に解決したアルゴリズムといえる \newline \newline
		$\Rightarrow$ CoCoA+や、それ以外の通信効率の良い(Communication-Efficient)分散アルゴリズムでも、収束していない(学習が上手く行っていない) \newline \newline
		$\Rightarrow$ 学習が上手く行かないのは、他のアルゴリズムでは、全てのデバイスが独立同分布なデータをもつと仮定しているため
		\newline
		
		\item FSVRGとFSVRGRの結果の差はごく僅かであり、Naive Federated SVRGを改良することで、各デバイスが持つデータの分布に左右されない(ロバストな)アルゴリズムを得られた
	\end{itemize}
\end{itemize}

\end{frame}

\section{結論}

\begin{frame}{結論}

\begin{itemize}
	\item Federated Learningの特徴
	\begin{itemize}
		\item \alert{Massively Distributed}、\alert{Non-IID}、\alert{Unbalanced}、\alert{Privacy Sensitive}、\alert{Sparse}の5つが挙げられる
		\newline
		\item これらの性質をもつ厳しい環境下においても、効率的に学習が進むアルゴリズムを設計可能
	\end{itemize} \
	
	\item 今後の研究の方向性について
	\begin{itemize}
		\item 非同期版のアルゴリズムの開発や、アルゴリズムの理論的な解明(特に収束性)
		\item 非凸関数の最適化(ニューラルネットが代表例)についての理論的な解明
		\item 全ユーザ間で共有されるモデルと、個々のユーザに特化したモデル双方の活用
	\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{参考文献}

\bibliographystyle{plain}
\bibliography{federated-optimization-bibliography}

\end{frame}

\end{document}
